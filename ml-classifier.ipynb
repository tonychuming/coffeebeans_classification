{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.impute import KNNImputer\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier\nimport itertools\nfrom tabulate import tabulate\nimport xgboost as xgb\nimport os\nfrom tqdm import tqdm\nfrom multiprocessing import Pool\nfrom sklearn.preprocessing import LabelEncoder\nfrom openpyxl import Workbook\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set the batch size for reading large datasets. \n# adjust based on available memory\nbatch_size = 30000","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# directory containing the csv files with features extracted from images\ncsv_directory = '/Users/tony/Desktop/coffeebeans/DataSet'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# list of csv files containing different sets of features\ncsv_files = ['original_features.csv', \n             'original_features_augmented.csv', \n             'denoised_features.csv', \n             'denoised_features_augmented.csv']\n\ndfs = {}\nfor file in csv_files:\n    file_path = os.path.join(csv_directory, file) # Construct full path for each file\n    try:\n        df_list = [] # To store chunks of the dataframe\n        with tqdm(total=os.path.getsize(file_path), # Get file size for progress bar\n                  desc=f'Reading {file}', # Description for the progress bar\n                  unit='B', # Unit of measurement for progress bar\n                  unit_scale=True, # Scale unit measurement\n                  bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}{postfix}]',\n                  colour='GREEN') as pbar: # Set color for the progress bar\n            bytes_read = 0\n            for chunk in pd.read_csv(file_path, chunksize=batch_size):\n                df_list.append(chunk) # Append each chunk to the list\n                bytes_read += chunk.memory_usage(deep=True).sum()\n                pbar.update(bytes_read - pbar.n)  # Update progress bar\n        dfs[file] = pd.concat(df_list, ignore_index=True) # Concatenate all chunks into a single dataframe\n    except FileNotFoundError:\n        print(f\"Error: CSV file not found: {file_path}\") # Handle missing file error\n        exit(1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List of classifiers to be used for training and evaluation\nmodels = [\n    RandomForestClassifier(max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=300, random_state=42),\n\n    KNeighborsClassifier(n_neighbors=10, weights='distance'),\n\n    DecisionTreeClassifier(max_depth=10, min_samples_leaf=1, min_samples_split=5, random_state=42),\n\n    GradientBoostingClassifier(learning_rate=0.2, max_depth=5, n_estimators=300, random_state=42),\n\n    ExtraTreesClassifier(max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=300, random_state=42),\n    \n    xgb.XGBClassifier(learning_rate=0.1, max_depth=10, n_estimators=300, eval_metric='logloss', random_state=42)\n]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate a trained model using multiple metrics\ndef evaluate_model(model, X_test, y_test):\n    \"\"\"\n    Evaluate a model on test data using accuracy, precision, recall, F1 score, and mean squared error.\n    \n    Parameters:\n    - model: Trained model to be evaluated\n    - X_test: Features of the test dataset\n    - y_test: True labels of the test dataset\n    \n    Returns:\n    - Tuple containing the evaluation metrics (accuracy, precision, recall, F1 score, MSE)\n    \"\"\"\n    try:\n        y_pred = model.predict(X_test)\n        accuracy = accuracy_score(y_test, y_pred)\n        precision = precision_score(y_test, y_pred, average='weighted')\n        recall = recall_score(y_test, y_pred, average='weighted')\n        f1 = f1_score(y_test, y_pred, average='weighted')\n        mse = mean_squared_error(y_test, y_pred)\n        return accuracy, precision, recall, f1, mse\n    except ValueError as e:\n        print(f\"Error: Invalid input for evaluation metrics: {str(e)}\")\n        return None","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract different types of features from the dataset columns\ndef extract_features(X):\n    \"\"\"\n    extracts different types of features (texture, color, shape) from the dataset\n    \n    parameters:\n        X: DataFrame containing the features\n\n    returns:\n        three lists containing the names of texture, color and shape features respectively\n    \"\"\"\n    texture_features = [col for col in X.columns \n                        if 'LBP_Pattern_' in col \n                        or 'GLCM_Feature_' in col \n                        or 'HOG_' in col \n                        or 'Gabor_Feature_' in col]\n    \n    color_features = [col for col in X.columns \n                      if 'Color_Moment_' in col \n                      or 'Color_Histogram_' in col \n                      or 'Color_Coherence_Vector_' in col\n                      or 'Color_Name_Histogram_' in col]\n    \n    shape_features = [col for col in X.columns \n                      if 'Hu_Moment_' in col \n                      or 'Contour_' in col \n                      or 'Fourier_Descriptor_' in col \n                      or 'Zernike_Moment_' in col]\n    \n    return texture_features, color_features, shape_features","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to train and evaluate models without k-fold cross-validation\ndef train_and_evaluate_models(X_train, y_train, X_test, y_test, models):\n    \"\"\"\n    Trains and evaluates a list of models on training and test datasets.\n    \n    Parameters:\n    - X_train: Training features\n    - y_train: Training labels\n    - X_test: Test features\n    - y_test: Test labels\n    - models: List of models to train and evaluate\n    \n    Returns:\n    - List of evaluation results for each model\n    \"\"\"\n    results = []\n    for model in models:\n        model.fit(X_train, y_train) # Train model\n        accuracy, precision, recall, f1, mse = evaluate_model(model, X_test, y_test) # Evaluate model\n        results.append([type(model).__name__, accuracy, precision, recall, f1, mse])\n    return results","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train and evaluate models using k-fold cross-validation\ndef train_and_evaluate_models_kfold(X, y, models, k=5):\n    \"\"\"\n    Trains and evaluates models using k-fold cross-validation.\n    \n    Parameters:\n    - X: Features of the dataset\n    - y: Labels of the dataset\n    - models: List of models to train and evaluate\n    - k: Number of folds for cross-validation (default: 5)\n    \n    Returns:\n    - List of average evaluation metrics for each model across k folds\n    \"\"\"\n    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n    results = []\n    for model in models:\n        accuracy_scores = []\n        precision_scores = []\n        recall_scores = []\n        f1_scores = []\n        mse_scores = []\n        for train_index, test_index in kf.split(X):\n            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n            y_train, y_test = y[train_index], y[test_index]\n            model.fit(X_train, y_train) # Train model on each fold\n            accuracy, precision, recall, f1, mse = evaluate_model(model, X_test, y_test) # Evaluate model\n            accuracy_scores.append(accuracy)\n            accuracy_scores.append(accuracy)\n            precision_scores.append(precision)\n            recall_scores.append(recall)\n            f1_scores.append(f1)\n            mse_scores.append(mse)\n\n        # Store average results across all folds\n        results.append([\n            type(model).__name__,\n            np.mean(accuracy_scores),\n            np.mean(precision_scores),\n            np.mean(recall_scores),\n            np.mean(f1_scores),\n            np.mean(mse_scores)\n        ])\n    return results","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_features(file, df, workbook):\n    \"\"\"\n    Evaluate and compare various feature extraction methods on the dataset, \n    both with and without k-fold cross-validation, and save the results to an Excel workbook.\n    \n    Parameters:\n    - file: The name of the dataset file being evaluated.\n    - df: The DataFrame containing the dataset.\n    - workbook: The Excel workbook where results will be saved.\n    \"\"\"\n    worksheet = workbook.create_sheet(title=file)\n\n    # Check for missing values in the dataset\n    if df.isnull().values.any():\n        print(\"Warning: Missing values detected in the dataset.\")\n        \n        # Impute missing values using KNN imputer\n        imputer = KNNImputer(n_neighbors=5)\n        df_imputed = imputer.fit_transform(df)\n        \n        # Convert the imputed array back to a DataFrame\n        df = pd.DataFrame(df_imputed, columns=df.columns)\n        print(\"Missing values have been imputed using KNN.\")\n    \n    # Separate features (X) and labels (y)\n    try:\n        X = df.drop(['path', 'filename', 'label'], axis=1) # Drop non-feature columns\n        y = df['label']\n    except KeyError as e:\n        print(f\"Error: Required column not found in the dataset: {str(e)}\")\n        return\n\n    # Encode labels to numerical format\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(y)\n    \n    # Extract different sets of features (texture, color, shape)\n    texture_features, color_features, shape_features = extract_features(X)\n    \n    # Split the dataset into training and testing sets\n    try:\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    except ValueError as e:\n        print(f\"Error: Invalid input for train-test split: {str(e)}\")\n        return\n    \n    # # # Evaluate each feature extraction method without k-fold cross-validation\n    best_features = {'texture': None, 'color': None, 'shape': None}\n    best_scores = {'texture': 0, 'color': 0, 'shape': 0}\n    \n    row = 1 # Start writing to Excel from the first row\n    for feature_set, features in zip(['Texture', 'Color', 'Shape'], [texture_features, color_features, shape_features]):\n        worksheet.cell(row=row, column=1, value=f\"Feature set: {feature_set}\")\n        print(f\"\\nFeature set: {feature_set}\")\n        row += 1\n\n        # Group features by their extraction method\n        feature_methods = {}\n        for feature in features:\n            method = '_'.join(feature.split('_')[:-1])\n            if method not in feature_methods:\n                feature_methods[method] = [feature]\n            else:\n                feature_methods[method].append(feature)\n        \n        # Evaluate models on each feature extraction method\n        for method, method_features in feature_methods.items():\n            print(f\"\\nFeature extraction: {method}\")\n            worksheet.cell(row=row, column=1, value=f\"Feature extraction: {method}\")\n            row += 1\n\n            X_train_subset = X_train[method_features]\n            X_test_subset = X_test[method_features]\n            \n            results = train_and_evaluate_models(X_train_subset, y_train, X_test_subset, y_test, models)\n\n            print(tabulate(results, headers=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"MSE\"], tablefmt=\"grid\"))\n\n            for r in results:\n                worksheet.append([''] + r)\n            row += len(results) + 1\n\n            # Track the best-performing feature method\n            best_model_index = np.argmax([r[1] for r in results])\n            if results[best_model_index][1] > best_scores[feature_set.lower()]:\n                best_scores[feature_set.lower()] = results[best_model_index][1]\n                best_features[feature_set.lower()] = method\n    \n    # Output the best feature methods found without k-fold\n    print(\"\\nBest features (without k-fold):\")\n    for category, method in best_features.items():\n        print(f\"{category.capitalize()}: {method}\")\n\n    worksheet.cell(row=row, column=1, value=\"Best features (without k-fold):\")\n    row += 1\n    for category, method in best_features.items():\n        worksheet.cell(row=row, column=1, value=f\"{category.capitalize()}: {method}\")\n        row += 1\n\n    # # # Evaluate each feature extraction method with k-fold cross-validation\n    best_features_kfold = {'texture': None, 'color': None, 'shape': None}\n    best_scores_kfold = {'texture': 0, 'color': 0, 'shape': 0}\n    \n    row += 1 # Add a blank row before the next section\n    for feature_set, features in zip(['Texture', 'Color', 'Shape'], [texture_features, color_features, shape_features]):\n        print(f\"\\nFeature set: {feature_set} (with k-fold)\")\n        worksheet.cell(row=row, column=1, value=f\"Feature set: {feature_set} (with k-fold)\")\n        row += 1 \n\n        # Group features by their extraction method\n        feature_methods = {}\n        for feature in features:\n            method = '_'.join(feature.split('_')[:-1])\n            if method not in feature_methods:\n                feature_methods[method] = [feature]\n            else:\n                feature_methods[method].append(feature)\n        \n        # Evaluate models on each feature extraction method with k-fold\n        for method, method_features in feature_methods.items():\n            print(f\"\\nFeature extraction: {method}\")\n            worksheet.cell(row=row, column=1, value=f\"Feature extraction: {method}\")\n            row += 1\n\n            X_subset = X[method_features]\n            \n            results = train_and_evaluate_models_kfold(X_subset, y, models)\n\n            print(tabulate(results, headers=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"MSE\"], tablefmt=\"grid\"))\n\n            for r in results:\n                worksheet.append([''] + r)\n            row += len(results) + 1\n\n            # Track the best-performing feature method with k-fold\n            best_model_index = np.argmax([r[1] for r in results])\n            if results[best_model_index][1] > best_scores_kfold[feature_set.lower()]:\n                best_scores_kfold[feature_set.lower()] = results[best_model_index][1]\n                best_features_kfold[feature_set.lower()] = method\n    \n    # Output the best feature methods found with k-fold\n    print(\"\\nBest features (with k-fold):\")\n    for category, method in best_features_kfold.items():\n        print(f\"{category.capitalize()}: {method}\")\n\n    worksheet.cell(row=row, column=1, value=\"Best features (with k-fold):\")\n    row += 1\n    for category, method in best_features_kfold.items():\n        worksheet.cell(row=row, column=1, value=f\"{category.capitalize()}: {method}\")\n        row += 1\n\n    # Generate combinations of best features for evaluation\n    best_feature_combinations = []\n    for r in range(1, len(best_features) + 1):\n        combinations = itertools.combinations(best_features.items(), r)\n        for combination in combinations:\n            feature_combination = []\n            combination_name = \"test_\" + \"_\".join([f\"{category}_{method}\" for category, method in combination])\n            for category, method in combination:\n                feature_combination.extend([col for col in X.columns if method in col])\n            best_feature_combinations.append((combination_name, feature_combination))\n    \n    # Evaluate best feature combinations without k-fold cross-validation\n    row += 1\n    print(\"\\nBest feature combinations (without k-fold):\")\n    worksheet.cell(row=row, column=1, value=\"Best feature combinations (without k-fold):\")\n    row += 1\n    for combination_name, features in best_feature_combinations:\n        print(f\"\\nFeature combination: {combination_name}\")\n        worksheet.cell(row=row, column=1, value=f\"Feature combination: {combination_name}\")\n        row += 1\n\n        X_train_subset = X_train[features]\n        X_test_subset = X_test[features]\n        \n        results = train_and_evaluate_models(X_train_subset, y_train, X_test_subset, y_test, models)\n\n        print(tabulate(results, headers=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"MSE\"], tablefmt=\"grid\"))\n\n        for r in results:\n            worksheet.append([''] + r)\n        row += len(results) + 1\n\n    # Evaluate best feature combinations with k-fold cross-validation\n    print(\"\\nBest feature combinations (with k-fold):\")\n    worksheet.cell(row=row, column=1, value=\"Best feature combinations (with k-fold):\")\n    row += 1\n    for combination_name, features in best_feature_combinations:\n        print(f\"\\nFeature combination: {combination_name}\")\n        worksheet.cell(row=row, column=1, value=f\"Feature combination: {combination_name}\")\n        row += 1\n\n        X_subset = X[features]\n        \n        results = train_and_evaluate_models_kfold(X_subset, y, models)\n\n        print(tabulate(results, headers=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"MSE\"], tablefmt=\"grid\"))\n\n        for r in results:\n            worksheet.append([''] + r)\n        row += len(results) + 1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n    \"\"\"\n    Main execution block for the feature evaluation script.\n    \"\"\"\n    workbook = Workbook() # Initialize an Excel workbook to store results\n\n    # Iterate over each dataset, evaluating features and saving results\n    for file, df in dfs.items():\n        print(f\"\\n=============== {file} ===============\")\n        evaluate_features(file, df, workbook)# Evaluate features for each file\n\n    # Clean up the default sheet created by openpyxl\n    workbook.remove(workbook['Sheet'])\n    workbook.save(\"classification_results.xlsx\") # Save the workbook as an Excel file","metadata":{},"execution_count":null,"outputs":[]}]}