{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import cv2 # OpenCV for image processing \n","import pandas as pd # For handling data frames\n","import numpy as np # Numpy for numerical operations \n","import os # For operating system interactions \n","from sklearn.cluster import KMeans # K-means clustering from scikit-learn\n","import time # To track the time taken for processes\n","from skimage.feature import graycomatrix, graycoprops, local_binary_pattern, hog # For texture and shape feature extraction\n","from scipy.stats import skew, kurtosis # For statistical features like skewness and kurtosis\n","import mimetypes # To guess the MIME type of files\n","from joblib import Parallel, delayed # For parallel processing \n","from tqdm import tqdm # To show progress bars for loops \n","from skimage.filters import gabor # For Gabor filter feature extraction\n","import mahotas  # Library for Zernike moments\n","import csv  # Import the csv module for handling CSV files\n","import gc # Import the gc module for garbage collection"]},{"cell_type":"markdown","metadata":{},"source":["# Preprocess Images"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def preprocess_image(image_path, apply_denoising):\n","    \"\"\"\n","    Preprocess the image by resizing and optionally applying denoising \n","    Parameters:\n","    image_path(str): The file path to the image\n","    apply_denosing (bool)ï¼šwhether to apply bilateral filtering for noise reduction\n","    Returns:\n","    image(np.array): the processed image\n","    gray(np.array): the grayscale version of the image\n","    \"\"\"\n","    try:\n","        image = cv2.imread(image_path) # Read the image from the given path\n","        if image is None:\n","            raise ValueError(f\"Error loading image: {image_path}\")\n","\n","        image = cv2.resize(image, (256, 256))  # Resize the image to 256x256 pixels\n","\n","        if apply_denoising:\n","            image = cv2.bilateralFilter(image, 9, 75, 75)  # Apply bilateral filtering to reduce noise\n","\n","        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # Convert the image to grayscale\n","        return image, gray\n","    except Exception as e:\n","        print(f\"Error preprocessing image {image_path}: {e}\")\n","        return None, None"]},{"cell_type":"markdown","metadata":{},"source":["# Texture Features"]},{"cell_type":"markdown","metadata":{},"source":["Local Binary Pattern (LBP)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def extract_lbp_features(gray, radius=2, n_points=16, method='uniform'):\n","    \"\"\"\n","    Extract Local Binary Pattern (LBP) features from a grayscale imgae\n","    Parameters:\n","    gray (np.array): the grayscale image\n","    radius (int): radius of the circular neighborhood\n","    n_points (int): number of points to consider for LBP\n","    method (str): method for LBP ('uniform' or other)\n","    Returns:\n","    hist (list): normalized histogram of LBP features\n","    \"\"\"\n","    try:\n","        lbp = local_binary_pattern(gray, n_points, radius, method) # compute LBP\n","        hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, n_points + 3), range=(0, n_points + 2)) # Histogram of LBP\n","        hist = hist.astype(np.float32)\n","        hist /= hist.sum() # Normalize the histogram\n","        return hist.tolist()\n","    except Exception as e:\n","        print(f\"Error extracting LBP features: {e}\")\n","        return [0] * (n_points + 2)"]},{"cell_type":"markdown","metadata":{},"source":["Gray Level Co-occurrence Matrix (GLCM)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def extract_glcm_features(gray, distances=[1, 2], angles=[0, np.pi/4]):\n","    \"\"\"\n","    Extract Gray Level Co-occurrence Matrix (GLCM) features from a grayscale image\n","    Parameters:\n","    gray (np.array): the grayscale image\n","    distances (list): list of distances for GLCM computation\n","    angles (list): list of angles for GLCM computation\n","    Returns:\n","    features (list): list of GLCM features including contrast, dissimilarity, homogeneity, energy, correlation, ASM\n","    \"\"\"\n","    features = []\n","    for d in distances:\n","        for a in angles:\n","            glcm = graycomatrix(gray, distances=[d], angles=[a], levels=256, symmetric=True, normed=True) # compute GLCM\n","            features.append(graycoprops(glcm, 'contrast')[0, 0]) # add contrast feature\n","            features.append(graycoprops(glcm, 'dissimilarity')[0, 0]) # add dissimilarity feature\n","            features.append(graycoprops(glcm, 'homogeneity')[0, 0]) #add homogeneity feature\n","            features.append(graycoprops(glcm, 'energy')[0, 0]) # add energy feature\n","            features.append(graycoprops(glcm, 'correlation')[0, 0]) # add correlation feature\n","            features.append(graycoprops(glcm, 'ASM')[0, 0]) # add ASM feature (angular second moment)\n","    return features"]},{"cell_type":"markdown","metadata":{},"source":["Histogram of Oriented Gradients (HOG)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def extract_hog_features(gray, pixels_per_cell=[(8, 8), (16, 16)], cells_per_block=[(2, 2), (3, 3)]):\n","    \"\"\"\n","    extract histogram of oriented gradients (HOG) features from a grayscale image\n","\n","    parameters:\n","    gray (np.array): the grayscale image\n","    pixels_per_cell (list): list of tuples specifiyign the size of each cell in pixels\n","    cells_per_block (list): list of tuples specifiying the number of cells in each block\n","\n","    returns:\n","    features (list): flattened list of hog features\n","    \"\"\"\n","    features = []\n","    for ppc in pixels_per_cell:\n","        for cpb in cells_per_block:\n","            hog_features = hog(gray, orientations=9, pixels_per_cell=ppc, cells_per_block=cpb, block_norm='L2-Hys', transform_sqrt=True)\n","            features.extend(hog_features) # append hog features\n","    return features"]},{"cell_type":"markdown","metadata":{},"source":["Gabor Filters"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def extract_gabor_features(gray, num_kernels=8):\n","    \"\"\"\n","    extract gabor filter features from a grayscale image\n","\n","    parameters:\n","    gray (np.array): the grayscale image\n","    num_kernels (int): number of gabor kernels to apply\n","\n","    returns:\n","    features (list): list of mean and variance values for each kernel\n","    \"\"\"\n","    features = []\n","    for i in range(num_kernels):\n","        theta = i * (np.pi / num_kernels)\n","        for frequency in [0.05, 0.1, 0.2, 0.3]:\n","            gabor_filt_real, _ = gabor(gray, frequency=frequency, theta=theta)\n","            features.extend([gabor_filt_real.mean(), gabor_filt_real.var()]) # append mean and variance of the filter response\n","    return features"]},{"cell_type":"markdown","metadata":{},"source":["# Color features"]},{"cell_type":"markdown","metadata":{},"source":["Color Moments"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def extract_color_moments(image):\n","    \"\"\"\n","    extract color moments (mean, standard deviation, skewness and kurtosis) from an image\n","    parameters:\n","    image (np.array): the input image\n","    returns:\n","    features (list): list of color moment features for each channel in different color spaces\n","    \"\"\"\n","    features = []\n","    for i in range(3):\n","        channel = image[:, :, i]\n","        features.append(np.mean(channel)) # mean\n","        features.append(np.std(channel)) # standard deviation\n","        features.append(skew(channel.flatten())) # skewness\n","        features.append(kurtosis(channel.flatten())) # kurtosis\n","\n","    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n","    for i in range(3):\n","        channel = hsv[:, :, i]\n","        features.append(np.mean(channel))\n","        features.append(np.std(channel))\n","        features.append(skew(channel.flatten()))\n","        features.append(kurtosis(channel.flatten()))\n","\n","    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n","    for i in range(3):\n","        channel = lab[:, :, i]\n","        features.append(np.mean(channel))\n","        features.append(np.std(channel))\n","        features.append(skew(channel.flatten()))\n","        features.append(kurtosis(channel.flatten()))\n","\n","    return features"]},{"cell_type":"markdown","metadata":{},"source":["Color Histogram"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def extract_color_histogram(image, bins=32):\n","    \"\"\"\n","    extract color histogram from an image in multiple color spaces\n","    parameters:\n","    image (np.array): the input image\n","    bins (int): number of bins for the histgram\n","    returns:\n","    features (list): flattened list of histogram features for each channel in different color space\n","    \"\"\"\n","    features = []\n","    for i in range(3):\n","        hist = cv2.calcHist([image], [i], None, [bins], [0, 256]) # compute histogram for each channel\n","        cv2.normalize(hist, hist) # normalize the histogram\n","        features.extend(hist.flatten())\n","\n","    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n","    for i in range(3):\n","        hist = cv2.calcHist([hsv], [i], None, [bins], [0, 256])\n","        cv2.normalize(hist, hist)\n","        features.extend(hist.flatten())\n","    \n","    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n","    for i in range(3):\n","        hist = cv2.calcHist([lab], [i], None, [bins], [0, 256])\n","        cv2.normalize(hist, hist)\n","        features.extend(hist.flatten())\n","\n","    return features"]},{"cell_type":"markdown","metadata":{},"source":["Color Coherence Vector"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def extract_color_coherence_vector(image, bins=32):\n","    \"\"\"\n","    extract the color coherence vector (CCV) from an image\n","    parameters:\n","    image (np.array): the input image\n","    bins (int): number of bins for the ccv\n","    returns:\n","    features (list): concatenated list of coherent and incoherent pixel counts for each bin\n","    \"\"\"\n","    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n","    features = []\n","\n","    for i in range(3):\n","        channel = hsv[:, :, i].astype(np.int16)  # convert pixel values to int16\n","        hist, _ = np.histogram(channel, bins=bins, range=(0, 256)) # compute histogram\n","        coherent_pixels = np.zeros(bins, dtype=np.int32)\n","        incoherent_pixels = np.zeros(bins, dtype=np.int32)\n","\n","        for j in range(1, channel.shape[0] - 1):\n","            for k in range(1, channel.shape[1] - 1):\n","                current_pixel = channel[j, k]\n","                bin_index = int(current_pixel * (bins - 1) // 255)\n","                \n","                if (np.abs(current_pixel - channel[j-1, k-1]) <= 1 and\n","                    np.abs(current_pixel - channel[j-1, k]) <= 1 and\n","                    np.abs(current_pixel - channel[j-1, k+1]) <= 1 and\n","                    np.abs(current_pixel - channel[j, k-1]) <= 1 and\n","                    np.abs(current_pixel - channel[j, k+1]) <= 1 and\n","                    np.abs(current_pixel - channel[j+1, k-1]) <= 1 and\n","                    np.abs(current_pixel - channel[j+1, k]) <= 1 and\n","                    np.abs(current_pixel - channel[j+1, k+1]) <= 1):\n","                    coherent_pixels[bin_index] += 1 # coherent pixel\n","                else:\n","                    incoherent_pixels[bin_index] += 1 # incoherent pixel\n","\n","        features.extend(coherent_pixels)\n","        features.extend(incoherent_pixels)\n","\n","    return features"]},{"cell_type":"markdown","metadata":{},"source":["Color Name Histogram"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# converting RGB to HEX\n","def RGB2HEX(color): \n","    \"\"\"\n","    convert an RGB color to HEX format\n","\n","    parameters:\n","    color (tuple): a tuple representing the RGB color\n","\n","    returns:\n","    str: the HEX representation of the color\n","    \"\"\"\n","    return \"#{:02x}{:02x}{:02x}\".format(int(color[0]), int(color[1]), int(color[2]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# assigning color names\n","def assign_color_name(color):\n","    \"\"\"\n","    assign a name to an RGB color based on the closest predefined color\n","\n","    parameters:\n","    color(tuple): a tuple representing the RGB color\n","\n","    returns:\n","    str: the name of the cloest color\n","    \"\"\"\n","    color_names = {\n","        '#000000': 'black',\n","        '#ffffff': 'white',\n","        '#ff0000': 'red',\n","        '#00ff00': 'green',\n","        '#0000ff': 'blue',\n","        '#ffff00': 'yellow',\n","        '#ff00ff': 'magenta',\n","        '#00ffff': 'cyan',\n","        '#800000': 'maroon',\n","        '#808000': 'olive',\n","        '#008000': 'green',\n","        '#800080': 'purple',\n","        '#008080': 'teal',\n","        '#000080': 'navy'\n","    }\n","    \n","    min_dist = float('inf')\n","    min_color = None\n","    \n","    for c, name in color_names.items():\n","        r, g, b = int(c[1:3], 16), int(c[3:5], 16), int(c[5:7], 16)\n","        dist = np.sqrt((color[0]-r)**2 + (color[1]-g)**2 + (color[2]-b)**2)\n","        if dist < min_dist:\n","            min_dist = dist\n","            min_color = name\n","    \n","    return min_color\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# extracting color name histogram\n","def extract_color_name_histogram(image, bins=14):\n","    \"\"\"\n","    extract a histogram of color names from an image\n","\n","    parameters:\n","    image(np.array): the input image\n","    bins(int): the number of color bins\n","\n","    returns:\n","    list: histogram of color names\n","    \"\"\"\n","    image_resized = cv2.resize(image, (256, 256), interpolation=cv2.INTER_NEAREST)\n","    image_hex = [RGB2HEX(color) for row in image_resized for color in row]\n","    \n","    color_names = [assign_color_name(list(int(h[i:i+2], 16) for i in (1, 3, 5))) for h in image_hex]\n","    \n","    color_name_hist = []\n","    for color_name in ['black', 'white', 'red', 'green', 'blue', 'yellow', 'magenta', 'cyan', 'maroon', 'olive', 'green', 'purple', 'teal', 'navy']:\n","        count = color_names.count(color_name)\n","        color_name_hist.append(count)\n","        \n","    return color_name_hist"]},{"cell_type":"markdown","metadata":{},"source":["# Shape features"]},{"cell_type":"markdown","metadata":{},"source":["Hu Moments"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def extract_hu_moments(gray):\n","    \"\"\"\n","    extract hu moments from a grayscale image, which are shape features invariant to scale, translation and rotation\n","\n","    parameters:\n","    gray (np.array): the grayscale image\n","\n","    returns:\n","    list: hu moments as a list of 7 values\n","    \"\"\"\n","    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n","    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","\n","    if contours:\n","        contour = max(contours, key=cv2.contourArea) # find the largest contour by area\n","        moments = cv2.moments(contour) # calculate moments \n","        hu_moments = cv2.HuMoments(moments) # calculate hu moments\n","        return list(hu_moments.flatten())\n","    else:\n","        return [0] * 7 # return a list of zeros if no contours are found"]},{"cell_type":"markdown","metadata":{},"source":["Morphological Features"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def extract_morphological_features(gray):\n","    \"\"\"\n","    extract morphological features such as area, perimeter, aspect ratio, circularity and solidity \n","\n","    parameters:\n","    gray(np.array): the grayscale image\n","\n","    returns:\n","    list: a list containing area, perimeter, aspect ratio, circularity and solidity\n","    \"\"\"\n","    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n","    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","\n","    if contours:\n","        contour = max(contours, key=cv2.contourArea) # find the largest contour by area\n","\n","        area = cv2.contourArea(contour) # calculate area\n","        perimeter = cv2.arcLength(contour, True) # calculate perimeter \n","        _, _, w, h = cv2.boundingRect(contour) # bounding rectangle to get width and height\n","        aspect_ratio = float(w) / h if h != 0 else 0 # aspect ratio (width/height)\n","        circularity = 4 * np.pi * area / (perimeter ** 2) if perimeter != 0 else 0 # circularity \n","\n","        hull = cv2.convexHull(contour) # convex hull\n","        hull_area = cv2.contourArea(hull) # hull area\n","        solidity = float(area) / hull_area if hull_area != 0 else 0 # solidity \n","\n","        return [area, perimeter, aspect_ratio, circularity, solidity]\n","    else:\n","        return [0] * 5 # return a list of zeros if no contours are found "]},{"cell_type":"markdown","metadata":{},"source":["Fourier Descriptors"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def extract_fourier_descriptor(gray, n_descriptors=32):\n","    \"\"\"\n","    extract fourier descriptors from a grayscale image for shape analysis\n","\n","    parameters:\n","    gray(np.array): the grayscale image\n","    n_descriptors(int): number of fourier descriptors to extract\n","\n","    returns:\n","    list: list of fourier descriptors\n","    \"\"\"\n","    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n","    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","\n","    if contours:\n","        contour = max(contours, key=cv2.contourArea) # find the largest contour by area\n","        contour = contour.reshape(-1, 2) # reshape for fourier transform \n","        contour_complex = np.empty(contour.shape[:-1], dtype=complex) \n","        contour_complex.real = contour[:, 0]\n","        contour_complex.imag = contour[:, 1]\n","        fourier_result = np.fft.fft(contour_complex) # compute fourier descriptors\n","        fourier_result = fourier_result[:n_descriptors] \n","        fourier_result = np.fft.fftshift(fourier_result) # shift zero frequency to the center\n","        return np.abs(fourier_result).tolist()\n","    else:\n","        return [0] * n_descriptors # return a list of zeros if no contours are found\n"]},{"cell_type":"markdown","metadata":{},"source":["Zernike Moments"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def extract_zernike_moments(gray, radius, degree):\n","    \"\"\"\n","    extract zernike moments from a grayscale image, which are useful for shape recognition\n","    parameters:\n","    gray(np.array): the grayscale image\n","    radius(int): radius of the circular region for moment calculation\n","    degree(int): the degree of the zernike moments\n","    returns:\n","    list: zernike moments as a list of values\n","    \"\"\"\n","    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n","    moments = mahotas.features.zernike_moments(binary, radius, degree) # compute zernike moments\n","    return moments.tolist()"]},{"cell_type":"markdown","metadata":{},"source":["# Extracting All Features from an Image"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def extract_features_from_image(image, gray):\n","    \"\"\"\n","    extract a comprehensive set of features from an imagem, including texture, color and shape features\n","\n","    parameters:\n","    image(np.array): the input image\n","    gray(np.array): the grayscale version of the image\n","\n","    returns:\n","    dict: a dictionary containing all the extract features\n","    \"\"\"\n","    features = {}\n","\n","    # Texture features\n","    lbp_features = extract_lbp_features(gray, radius=1, n_points=8, method='uniform')\n","    if lbp_features is not None:\n","        for i, value in enumerate(lbp_features):\n","            features[f'LBP_Pattern_{i}'] = value\n","\n","    glcm_features = extract_glcm_features(gray, distances=[1, 2, 3], angles=[0, np.pi/4, np.pi/2, 3*np.pi/4])\n","    for i, value in enumerate(glcm_features):\n","        features[f'GLCM_Feature_{i}'] = value\n","\n","    hog_features = extract_hog_features(gray, pixels_per_cell=[(16, 16)], cells_per_block=[(3, 3)])\n","    for i, value in enumerate(hog_features):\n","        features[f'HOG_{i}'] = value\n","\n","    gabor_features = extract_gabor_features(gray, num_kernels=16)\n","    for i, value in enumerate(gabor_features):\n","        features[f'Gabor_Feature_{i}'] = value\n","\n","    # Color features\n","    color_moments = extract_color_moments(image)\n","    for i, value in enumerate(color_moments):\n","        features[f'Color_Moment_{i}'] = value\n","\n","    color_histogram = extract_color_histogram(image, bins=16)\n","    for i, value in enumerate(color_histogram):\n","        features[f'Color_Histogram_{i}'] = value\n","\n","    color_coherence_vector = extract_color_coherence_vector(image, bins=16)\n","    for i, value in enumerate(color_coherence_vector):\n","        features[f'Color_Coherence_Vector_{i}'] = value\n","\n","    color_name_histogram = extract_color_name_histogram(image, bins=14)\n","    for i, value in enumerate(color_name_histogram):\n","        features[f'Color_Name_Histogram_{i}'] = value\n","\n","    # Shape features\n","    hu_moments = extract_hu_moments(gray)\n","    for i, value in enumerate(hu_moments):\n","        features[f'Hu_Moment_{i}'] = value\n","\n","    morphological_features = extract_morphological_features(gray)\n","    features['Contour_Area'] = morphological_features[0]\n","    features['Contour_Perimeter'] = morphological_features[1]\n","    features['Contour_AspectRatio'] = morphological_features[2]\n","    features['Contour_Circularity'] = morphological_features[3]\n","    features['Contour_Solidity'] = morphological_features[4]\n","\n","    fourier_descriptor = extract_fourier_descriptor(gray, n_descriptors=16)\n","    for i, value in enumerate(fourier_descriptor):\n","        features[f'Fourier_Descriptor_{i}'] = value\n","\n","    zernike_moments = extract_zernike_moments(gray, radius=128, degree=4)\n","    for i, value in enumerate(zernike_moments):\n","        features[f'Zernike_Moment_{i}'] = value\n","\n","    return features"]},{"cell_type":"markdown","metadata":{},"source":["# Extracting Features from an Image Path"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def extract_features(image_path, apply_denoising=False, augment=False):\n","    \"\"\"\n","    extract features from an image located at the given path. this includes optional preprocessing steps like denoising and augmentation\n","\n","    parameters:\n","    image_path(str): the file path to the image\n","    apply_denoising(bool): whether to apply denoising using bilateral filtering \n","    augment(bool): whether to apply data augmentation techniques, such as horizontal flipping\n","\n","    returns:\n","    dict: a dictionary containing all the extracted features\n","    \"\"\"\n","    try:\n","        image_path = str(image_path) # ensure the image path is a string \n","\n","        if not os.path.exists(image_path):\n","            print(f\"Image not found: {image_path}\")\n","            return {}\n","\n","        image, gray = preprocess_image(image_path, apply_denoising) # preprocess the image (resize, denoise, grayscale)\n","        if image is None or gray is None:\n","            return {}\n","\n","        features = extract_features_from_image(image, gray) # extract all features from the preprocessed image\n","\n","        if augment:\n","            # apply horizontal flip augmentation if specified\n","            image_flipped = cv2.flip(image, 1) #flip the image horizontally\n","            gray_flipped = cv2.flip(gray, 1) # flip the grayscale image horizontally\n","\n","            # extract features from the flipped image\n","            features_flipped = extract_features_from_image(image_flipped, gray_flipped)\n","\n","            # add the flipped image features to the original features, prefixed with 'flipped_'\n","            features.update({f'flipped_{k}': v for k, v in features_flipped.items()})\n","\n","        return features\n","    except Exception as e:\n","        print(f\"Error processing image {image_path}: {str(e)}\")\n","        return {}"]},{"cell_type":"markdown","metadata":{},"source":["# Main Execution "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if __name__ == '__main__':\n","    \"\"\"\n","    main block of the script\n","    it handles the process of extracting features from images in a dataset and saving them to csv files\n","    \"\"\"\n","\n","    root_path = '/Users/tony/Desktop/coffeebeans/DataSet' #define the root path where the dataset is stored\n","    os.chdir(root_path) # change the current working directory to the dataset path \n","\n","    label, path, names = [], [], [] # initialize lists to hold image paths, labels and filenames\n","\n","    # Iterate through each folder in the dataset directory\n","    for folder_name in os.listdir(root_path):\n","        if folder_name not in ['.DS_Store', 'code_oldversion', '.git']: # skip unnecessary directories\n","            current_dir = os.path.join(root_path, folder_name) # construct the full path to the current dirctory\n","            for root, dirs, files in os.walk(current_dir): # walk through the directory tree\n","                for file in files:\n","                    file_mimetype = mimetypes.guess_type(file)[0] # guess the file's MIME type\n","                    if file_mimetype == 'image/jpeg': # process only JPEG images\n","                        path.append(os.path.join(root, file)) # store the full file path \n","                        label.append(folder_name) # store the lable (which is the folder name)\n","                        names.append(file) # store the filename\n","\n","    # create a dataframe to organize the data\n","    df = pd.DataFrame({'path': path, \n","                       'filename': names, \n","                       'label': label})\n","    \n","    print(df.head()) # display the first few rows of the dataframe\n","    print(df.tail()) # display the last few rows of the dataframe\n","\n","    # define the names of the csv files where the features will be saved\n","    csv_names = ['original_features.csv', \n","                 'original_features_augmented.csv', \n","                 'denoised_features.csv', \n","                 'denoised_features_augmented.csv']\n","\n","    # adjust batch_size based on the memory available on the macbook\n","    batch_size = 2000\n","\n","    # iterate over the different combinations of preprocessing options (denoising and augmentation)\n","    for i, (apply_denoising, augment) in enumerate([(False, False), (False, True), (True, False), (True, True)]):\n","        with open(csv_names[i], 'w', newline='') as csv_file: # open the corresponding csv file for writing\n","            # prepare the header of the csv file, including the feature names\n","            fieldnames = ['path', 'filename', 'label'] + [f for f in extract_features(df['path'][0], apply_denoising, augment).keys()]\n","            writer = csv.DictWriter(csv_file, fieldnames=fieldnames) # initialize the csv writer\n","            writer.writeheader() # write the header row\n","\n","            # process images in batches to manage memory usage\n","            for batch_start in range(0, len(df), batch_size):\n","                batch_end = min(batch_start + batch_size, len(df)) #determine the end index of the batch\n","                batch_df = df[batch_start:batch_end] # extract the current batch from the dataframe\n","\n","                start_time = time.time() # record the start time of the batch processing \n","                features_list = Parallel(n_jobs=-1)(delayed(extract_features)(path, \n","                apply_denoising, augment) for path in tqdm(batch_df['path'], desc=f\"Processing images ({'original' if not apply_denoising else 'denoised'}\n","                                                           {', augmented' if augment else ''}, batch {batch_start // batch_size + 1})\"))\n","\n","                for feature_dict in features_list:\n","                    if feature_dict:\n","                        row = {**batch_df.iloc[features_list.index(feature_dict)].to_dict(), **feature_dict} # merge image metadata with extracted features\n","                        writer.writerow(row) # write the combined data to the csv file\n","  \n","                end_time = time.time() # record the end time of the batch processing \n","                print(f\"Finished processing batch {batch_start // batch_size + 1} in {end_time - start_time:.2f} seconds.\")# print the time taken for this batch \n","\n","                gc.collect()  # manually trigger garbage collection to free memory \n","\n","        print(f\"Finished processing all batches for {'original' if not apply_denoising else 'denoised'}{', augmented' if augment else ''} images.\") # comfirm completion of all batches for the curent csv\n","\n","    print(\"Finished initial processing of all images.\") # comfirm completion of all image processing \n","\n","    #check if any of the expected csv files are missing\n","    missing_files = [file for file in csv_names if not os.path.exists(file)]\n","\n","    if not missing_files:\n","        print(\"All expected CSV files have been generated successfully.\")\n","    else:\n","        print(f\"The following CSV files are missing: {', '.join(missing_files)}\")\n","        print(\"Regenerating missing files...\")\n","\n","        # regenerate missing csv files\n","        for i, (apply_denoising, augment) in enumerate([(False, False), (False, True), (True, False), (True, True)]):\n","            if csv_names[i] in missing_files:\n","                with open(csv_names[i], 'w', newline='') as csv_file:\n","                    fieldnames = ['path', 'filename', 'label'] + [f for f in extract_features(df['path'][0], apply_denoising, augment).keys()]\n","                    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n","                    writer.writeheader()\n","\n","                    for batch_start in range(0, len(df), batch_size):\n","                        batch_end = min(batch_start + batch_size, len(df))\n","                        batch_df = df[batch_start:batch_end]\n","\n","                        start_time = time.time()\n","                        features_list = Parallel(n_jobs=-1)(delayed(extract_features)(path, apply_denoising, augment) for path in tqdm(batch_df['path'], desc=f\"Regenerating features for {'original' if not apply_denoising else 'denoised'}{', augmented' if augment else ''} images, batch {batch_start // batch_size + 1}\"))\n","\n","                        for feature_dict in features_list:\n","                            if feature_dict:\n","                                row = {**batch_df.iloc[features_list.index(feature_dict)].to_dict(), **feature_dict}\n","                                writer.writerow(row)\n","\n","                        end_time = time.time()\n","                        print(f\"Finished regenerating batch {batch_start // batch_size + 1} in {end_time - start_time:.2f} seconds.\")\n","\n","                        gc.collect()  # manually trigger garbage collection \n","\n","                print(f\"Finished regenerating {csv_names[i]}.\")\n","\n","    print(\"Program completed.\") # comfirm the completion of the entire program"]}],"metadata":{"kernelspec":{"display_name":"newenv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":4}
